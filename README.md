# Learning Hadoop 2 [Video]
This is the code repository for [Learning Hadoop 2 [Video]](https://www.packtpub.com/big-data-and-business-intelligence/building-hadoop-clusters-video?utm_source=github&utm_medium=repository&utm_campaign=9781783284030), published by [Packt](https://www.packtpub.com/?utm_source=github). It contains all the supporting project files necessary to work through the video course from start to finish.
## About the Video Course
Hadoop emerged in response to the proliferation of masses and masses of data collected by organizations, offering a strong solution to store, process, and analyze what has commonly become known as Big Data. It comprises a comprehensive stack of components designed to enable these tasks on a distributed scale, across multiple servers and thousands of machines. 

Learning Hadoop 2 introduces you to the powerful system synonymous with Big Data, demonstrating how to create an instance and leverage Hadoop ecosystem's many components to store, process, manage, and query massive data sets with confidence.

We open this course by providing an overview of the Hadoop component ecosystem, including HDFS, Sqoop, Flume, YARN, MapReduce, Pig, and Hive, before installing and configuring our Hadoop environment. We take a look at Hue, the graphical user interface of Hadoop.

We will then discover HDFS, Hadoop’s file-system used to store data. We will learn how to import and export data, both manually and automatically. Afterward, we turn our attention toward running computations using MapReduce, and get to grips working with Hadoop’s scripting language, Pig. Lastly, we will siphon data from HDFS into Hive, and demonstrate how it can be used to structure and query data sets.

<H2>What You Will Learn</H2>
<DIV class=book-info-will-learn-text>
<UL>
<LI>Explore Amazon's Web Services to manage big data 
<LI>Configure network and security settings when deploying instances to the cloud 
<LI>Explore methods to connect to cloud instances using your client machine 
<LI>Set up Linux environments and configure settings for services and package installations 
<LI>Examine Hadoop's general architecture and what each service brings to the table 
<LI>Harness and navigate Hadoop's file storage and processing mechanisms 
<LI>Install and master Apache Hadoop User Interface (HUE) </LI></UL></DIV>

## Instructions and Navigation
### Assumed Knowledge
To fully benefit from the coverage included in this course, you will need:<br/>
Low on theory, high on practice, this introduction to Hadoop delivers step-by-step guidance on setting up an instance, and working with each of the components of the Hadoop ecosystem. By the end of this course, you will be capable of implementing an Hadoop instance, storing, processing, and analyzing data with the framework.
### Technical Requirements
This course has the following software requirements:<br/>
Minimum RAM required: 4GB (Suggested: 8GB)
Minimum Free Disk Space: 25GB
Minimum Processor i3 or above
Operating System of 64bit (Suggested)

## Related Products
* [Hands-On Big Data Processing with Hadoop 3 [Video]](https://www.packtpub.com/big-data-and-business-intelligence/building-hadoop-clusters-video?utm_source=github&utm_medium=repository&utm_campaign=9781783284030)

* [The Ultimate Hands-on Hadoop [Video]](https://www.packtpub.com/big-data-and-business-intelligence/building-hadoop-clusters-video?utm_source=github&utm_medium=repository&utm_campaign=9781783284030)

* [Building Hadoop Clusters [Video]](https://www.packtpub.com/big-data-and-business-intelligence/building-hadoop-clusters-video?utm_source=github&utm_medium=repository&utm_campaign=9781783284030)

